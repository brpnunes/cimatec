{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrabalhoDataLinkage.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brpnunes/cimatec/blob/master/TrabalhoDataLinkage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHx4H0CcylJZ",
        "colab_type": "code",
        "outputId": "5df40192-c0cf-4452-c468-76f513faa7ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "#Instala e configura pyspark\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-2.4.2/spark-2.4.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.2-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "!pip install jellyfish\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-1.11.0-openjdk-amd64/\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.2-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jellyfish\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/9f/ae6f6ad509725b71d45bb408953c850da7a2ecc3dbdad4063a825702ba29/jellyfish-0.7.1.tar.gz (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: jellyfish\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/33/cf/c7ce9866a02202b1f8ca45595e20f5145bf56c0262cfa2daf1\n",
            "Successfully built jellyfish\n",
            "Installing collected packages: jellyfish\n",
            "Successfully installed jellyfish-0.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cksjeXlTl0K1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Funções de similaridade\n",
        "import jellyfish\n",
        "\n",
        "#Para os campos nome e mae, será usada comparação via jaro_winkler dos códigos fonéticos das strings\n",
        "def get_metaphone_similarity(s1, s2):\n",
        "  return jellyfish.jaro_winkler(jellyfish.metaphone(s1), jellyfish.metaphone(s2))\n",
        "\n",
        "#Para as datas, faremos a comparação como strings. Não calcularemos a similaridade usando o tipo data, porque uma simples troca de caracteres causada por erro de digitação poderia dar uma distância muito grande. Calcularemos a similaridade usando jaro_winkler e tratando como string.\n",
        "def get_string_similarity(s1, s2):\n",
        "  return jellyfish.jaro_winkler(s1, s2)\n",
        "\n",
        "#Caso a data de nascimento seja informada, a similaridade será calculada usando o nome, mae e  data_nasc, com maior peso para nome e data_nasc. Caso a data_nasc não seja informada, usaremos apenas o nome e mae, com maior peso para o nome.\n",
        "def compose_weighted_similarity_strategy_1(sim_nome, sim_mae, sim_data):\n",
        "  if (sim_data > 0):\n",
        "    return (2*sim_nome+ sim_mae +2*sim_data)/5\n",
        "  return (2*sim_nome+ sim_mae)/3\n",
        "\n",
        "#Caso a data de nascimento seja informada, a similaridade será calculada usando o nome, mae e  data_nasc, com maior peso para nome e data_nasc. Caso a data_nasc não seja informada, usaremos apenas o nome e mae, com maior peso para o nome.\n",
        "def compose_weighted_similarity_strategy_2(sim_nome, sim_mae, sim_data, sim_tipo_sanguineo):\n",
        "  if(sim_nome > 0):\n",
        "    if (sim_data > 0):\n",
        "      return (2*sim_nome+ sim_mae +2*sim_data)/5  \n",
        "    return (2*sim_nome+ sim_mae)/3\n",
        "  return (2*sim_tipo_sanguineo + sim_mae + 2*sim_data)/5\n",
        "\n",
        "def compose_weighted_similarity_strategy_3(sim_1, sim_2, sim_data):\n",
        "  if (sim_data > 0):\n",
        "    return (0.2*sim_1 + 0.1*sim_2 + 0.7*sim_data)\n",
        "  return 2/3*sim_1 + 1/3*sim_2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Funções \"split\"\n",
        "def criaUltimoNome(col):\n",
        "    return col.split(' ')[-1]\n",
        "\n",
        "def criaPrimeiroNome(col):\n",
        "    return col.split(' ')[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LtOK-NxLdaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cria as udfs\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "get_metaphone_similarity_udf = udf(get_metaphone_similarity, FloatType())\n",
        "\n",
        "get_string_similarity_udf = udf(get_string_similarity, FloatType())\n",
        "\n",
        "compose_weighted_similarity_strategy_1_udf = udf(compose_weighted_similarity_strategy_1, FloatType())\n",
        "\n",
        "compose_weighted_similarity_strategy_2_udf = udf(compose_weighted_similarity_strategy_2, FloatType())\n",
        "\n",
        "compose_weighted_similarity_strategy_3_udf = udf(compose_weighted_similarity_strategy_3, FloatType())\n",
        "\n",
        "udf_criaUltimoNome = F.udf(criaUltimoNome, StringType())\n",
        "\n",
        "udf_criaPrimeiroNome = F.udf(criaPrimeiroNome, StringType())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wJPTLe7jFrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Leitura das bases\n",
        "df_a = spark.read.csv(\"base_sintetica_ascii_a.csv\", sep=',', header=True)\n",
        "df_b = spark.read.csv(\"base_sintetica_ascii_b.csv\", sep=',', header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4sQIRDZjSS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import trim\n",
        "\n",
        "\n",
        "#Renomeia colunas para não haver conflito na hora do join\n",
        "df_a = df_a.withColumnRenamed(\"nome\", \"nome_a\")\n",
        "df_a = df_a.withColumnRenamed(\"mae\", \"mae_a\")\n",
        "df_a = df_a.withColumnRenamed(\"data_nasc\", \"data_nasc_a\")\n",
        "df_a = df_a.withColumnRenamed(\"cpf\", \"cpf_a\")\n",
        "df_a = df_a.withColumnRenamed(\"tipo_sanguineo\", \"tipo_sanguineo_a\")\n",
        "\n",
        "df_b = df_b.withColumnRenamed(\"nome\", \"nome_b\")\n",
        "df_b = df_b.withColumnRenamed(\"mae\", \"mae_b\")\n",
        "df_b = df_b.withColumnRenamed(\"data_nasc\", \"data_nasc_b\")\n",
        "df_b = df_b.withColumnRenamed(\"cpf\", \"cpf_b\")\n",
        "df_b = df_b.withColumnRenamed(\"tipo_sanguineo\", \"tipo_sanguineo_b\")\n",
        "\n",
        "\n",
        "#Preenche os campos não informados com a string \" \"\n",
        "df_a = df_a.na.fill({\"nome_a\": \" \", \"mae_a\" : \" \", \"data_nasc_a\": \" \", \"tipo_sanguineo_a\": \" \"})\n",
        "df_b = df_b.na.fill({\"nome_b\": \" \", \"mae_b\" : \" \", \"data_nasc_b\": \" \", \"tipo_sanguineo_b\": \" \"})\n",
        "\n",
        "#Faz o crossjoin entre as bases\n",
        "df_join = df_b.select(\"nome_b\", \"mae_b\", \"data_nasc_b\", \"cpf_b\", \"tipo_sanguineo_b\").crossJoin(df_a.select(\"nome_a\", \"mae_a\", \"data_nasc_a\", \"cpf_a\", \"tipo_sanguineo_a\"))\n",
        "\n",
        "df_join = df_join.withColumn('ultimo_nome_a', udf_criaUltimoNome(trim(F.col('nome_a'))))\n",
        "df_join = df_join.withColumn('ultimo_nome_b', udf_criaUltimoNome(trim(F.col('nome_b'))))\n",
        "\n",
        "df_join = df_join.withColumn('primeiro_nome_a', udf_criaPrimeiroNome(trim(F.col('nome_a'))))\n",
        "df_join = df_join.withColumn('primeiro_nome_b', udf_criaPrimeiroNome(trim(F.col('nome_b'))))\n",
        "\n",
        "df_join = df_join.withColumn('ultimo_nome_mae_a', udf_criaUltimoNome(trim(F.col('mae_a'))))\n",
        "df_join = df_join.withColumn('ultimo_nome_mae_b', udf_criaUltimoNome(trim(F.col('mae_b'))))\n",
        "\n",
        "df_join = df_join.withColumn('primeiro_nome_mae_a', udf_criaPrimeiroNome(trim(F.col('mae_a'))))\n",
        "df_join = df_join.withColumn('primeiro_nome_mae_b', udf_criaPrimeiroNome(trim(F.col('mae_b'))))\n",
        "\n",
        "\n",
        "#Cria as novas colunas com as similaridades\n",
        "df_join = df_join.withColumn(\"sim_nome\", get_metaphone_similarity_udf(df_join[\"nome_a\"], df_join[\"nome_b\"]))\n",
        "df_join = df_join.withColumn(\"sim_mae\", get_metaphone_similarity_udf(df_join[\"mae_a\"], df_join[\"mae_b\"]))\n",
        "df_join = df_join.withColumn(\"sim_data_nasc\", get_string_similarity_udf(df_join[\"data_nasc_a\"], df_join[\"data_nasc_b\"]))\n",
        "df_join = df_join.withColumn(\"sim_tipo_sanguineo\", get_string_similarity_udf(df_join[\"tipo_sanguineo_a\"], df_join[\"tipo_sanguineo_b\"]))\n",
        "df_join = df_join.withColumn(\"sim_primeiro_nome\", get_metaphone_similarity_udf(df_join[\"primeiro_nome_a\"], df_join[\"primeiro_nome_b\"]))\n",
        "df_join = df_join.withColumn(\"sim_ultimo_nome\", get_metaphone_similarity_udf(df_join[\"ultimo_nome_a\"], df_join[\"ultimo_nome_b\"]))\n",
        "df_join = df_join.withColumn(\"sim_primeiro_nome_mae\", get_metaphone_similarity_udf(df_join[\"primeiro_nome_mae_a\"], df_join[\"primeiro_nome_mae_b\"]))\n",
        "df_join = df_join.withColumn(\"sim_ultimo_nome_mae\", get_metaphone_similarity_udf(df_join[\"ultimo_nome_mae_a\"], df_join[\"ultimo_nome_mae_b\"]))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjZWf5ENmveN",
        "colab_type": "text"
      },
      "source": [
        "Estratégia 1: Utilizando a similaridade do nome, nome da mãe e data de nascimento "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFAswcHKj8bz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def do_linkage_strategy_1(df_join):\n",
        "  \n",
        "  #A coluna similarity guarda a similaridade ponderada considerando nome, mae e data_nasc\n",
        "  df_final = df_join.withColumn(\"similarity\", compose_weighted_similarity_strategy_1_udf(df_join[\"sim_nome\"], df_join[\"sim_mae\"], df_join[\"sim_data_nasc\"]))\n",
        "\n",
        "  #Ordena o dataset pela similaridade, agrupando pelo nome_b e mae_b\n",
        "  df_final = df_final.orderBy([\"cpf_b\", \"similarity\"], ascending=[1, 0])\n",
        "  \n",
        "  #Exclui as entradas duplicadas, mantendo apenas as linhas que tiverem a maior similaridade para cada nome_b e mae_b\n",
        "  df_final = df_final.dropDuplicates([\"cpf_b\"])\n",
        "\n",
        "  #Gera o dataset final e grava no arquivo results_for_accuracy.csv\n",
        "  df_final = df_final.toPandas()\n",
        "\n",
        "  df_final = df_final[[\"cpf_a\", \"cpf_b\", \"similarity\"]]\n",
        "\n",
        "  df_final.to_csv(\"results_for_accuracy_calc_strategy_1.csv\")\n",
        "\n",
        "  df_final.count()\n",
        " \n",
        "#do_linkage_strategy_1(df_join)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fziO7MkEm9j2",
        "colab_type": "text"
      },
      "source": [
        "Estratégia 2: Utilizando a similaridade do nome, nome da mãe, data de nascimento e tipo sanguíneo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usw4ijBc6Gwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def do_linkage_strategy_2(df_join):\n",
        "  \n",
        "\n",
        "  #A coluna similarity guarda a similaridade ponderada considerando nome, mae e data_nasc\n",
        "  df_final = df_join.withColumn(\"similarity\", compose_weighted_similarity_strategy_2_udf(df_join[\"sim_nome\"], df_join[\"sim_mae\"], df_join[\"sim_data_nasc\"], df_join[\"sim_tipo_sanguineo\"]))\n",
        "\n",
        "  #Ordena o dataset pela similaridade, agrupando pelo nome_b e mae_b\n",
        "  df_final = df_final.orderBy([\"cpf_b\", \"similarity\"], ascending=[1, 0])\n",
        "  \n",
        "  #Exclui as entradas duplicadas, mantendo apenas as linhas que tiverem a maior similaridade para cada nome_b e mae_b\n",
        "  df_final = df_final.dropDuplicates([\"cpf_b\"])\n",
        "\n",
        "  #Gera o dataset final e grava no arquivo results_for_accuracy.csv\n",
        "  df_final = df_final.toPandas()\n",
        "\n",
        "  df_final = df_final[[\"cpf_a\", \"cpf_b\", \"similarity\"]]\n",
        "\n",
        "  df_final.to_csv(\"results_for_accuracy_calc_strategy_2.csv\")\n",
        "\n",
        "  df_final.count()\n",
        " \n",
        "#do_linkage_strategy_2(df_join)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJL4_7fem_Mc",
        "colab_type": "text"
      },
      "source": [
        "Estratégia 3: Utilizando a similaridade do primeiro nome e do primeiro nome da mãe\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm9jDy-gJyDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def do_linkage_strategy_3(df_join):\n",
        "  \n",
        "\n",
        "  #A coluna similarity guarda a similaridade ponderada considerando nome, mae e data_nasc\n",
        "  df_final = df_join.withColumn(\"similarity\", compose_weighted_similarity_strategy_3_udf(df_join[\"sim_primeiro_nome\"], df_join[\"sim_primeiro_nome_mae\"], df_join[\"sim_data_nasc\"]))\n",
        "\n",
        "  #Ordena o dataset pela similaridade, agrupando pelo nome_b e mae_b\n",
        "  df_final = df_final.orderBy([\"cpf_b\", \"similarity\"], ascending=[1, 0])\n",
        "  \n",
        "  #Exclui as entradas duplicadas, mantendo apenas as linhas que tiverem a maior similaridade para cada nome_b e mae_b\n",
        "  df_final = df_final.dropDuplicates([\"cpf_b\"])\n",
        "\n",
        "  #Gera o dataset final e grava no arquivo results_for_accuracy.csv\n",
        "  df_final = df_final.toPandas()\n",
        "\n",
        "  df_final = df_final[[\"cpf_a\", \"cpf_b\", \"similarity\"]]\n",
        "\n",
        "  df_final.to_csv(\"results_for_accuracy_calc_strategy_3.csv\")\n",
        "\n",
        "  df_final.count()\n",
        " \n",
        "#do_linkage_strategy_2(df_join)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tYYcqbWXXS5",
        "colab_type": "text"
      },
      "source": [
        "Cálculo de acurácia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XClsZtzjgJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "import pandas as pd\n",
        "\n",
        "def inspect_pairs(cpf_a, cpf_b, match):\n",
        "    if match == '1':\n",
        "        if cpf_a == cpf_b:\n",
        "            return \"TP\"\n",
        "        else:\n",
        "            return \"FP\"\n",
        "    else:\n",
        "        if cpf_a != cpf_b:\n",
        "            return \"TN\"\n",
        "        else: \n",
        "            return \"FN\"\n",
        "          \n",
        "udf_inspect_pairs = F.udf(inspect_pairs, StringType())\n",
        "\n",
        "def get_results(filename):\n",
        "  data = spark.read.csv(filename, header=True)\n",
        "  \n",
        "  #O valor do ponto de corte (cutoff) deve ser o número que separa os 100  primeiros do restante de sua base que resultou do linkage  \n",
        "  cutoff = data.orderBy('similarity', ascending=False).toPandas().iloc[99, 3]\n",
        "  \n",
        "  \n",
        "  # Let us consider a cuttoff point set as 0.85\n",
        "  #cutoff = 0.9\n",
        "\n",
        "  # sorting and deduplicating the resulting dataset\n",
        "  data = data.withColumn('similarity', F.col('similarity').cast(DoubleType()))\n",
        "  data = data.orderBy('similarity', ascending=False).dropDuplicates(['cpf_b'])\n",
        "  \n",
        "  data = data.withColumn('match', F.when(F.col('similarity') >= cutoff, '1').otherwise('0'))\n",
        "  \n",
        "  data = data.withColumn('perf', udf_inspect_pairs(F.col('cpf_a'), F.col('cpf_b'), F.col('match')))\n",
        "    \n",
        "  dic_results = {}\n",
        "  TP = data.filter(F.col('perf') == \"TP\").count()\n",
        "  TN = data.filter(F.col('perf') == \"TN\").count()\n",
        "  FP = data.filter(F.col('perf') == \"FP\").count()\n",
        "  FN = data.filter(F.col('perf') == \"FN\").count()\n",
        "\n",
        "\n",
        "  dic_results['accuracy'] =  float(TP + TN) / (FP + TP + FN + TN)\n",
        "  dic_results['ppv'] = float(TP) / (TP + FP)\n",
        "  dic_results['npv'] = float(TN) / (TN + FN)\n",
        "  dic_results['sens'] = float(TP) / (TP + FN)\n",
        "  dic_results['spec'] = float(TN) / (TN + FP)\n",
        "  \n",
        "  final_results = pd.DataFrame(dic_results, index=[0])\n",
        "  \n",
        "  return final_results\n",
        "  \n",
        " \n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fr3TLgJWmbC",
        "colab_type": "text"
      },
      "source": [
        "3 Estratégias de Linkage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsGOXbO0kmvF",
        "colab_type": "code",
        "outputId": "45be071e-5205-4504-fbc0-a16821c15885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "do_linkage_strategy_1(df_join)\n",
        "print(\"Estratégia 1: \\n %s \" % get_results(\"results_for_accuracy_calc_strategy_1.csv\"))\n",
        "\n",
        "do_linkage_strategy_2(df_join)\n",
        "print(\"\\nEstratégia 2: \\n %s \" % get_results(\"results_for_accuracy_calc_strategy_2.csv\"))\n",
        "\n",
        "do_linkage_strategy_3(df_join)\n",
        "print(\"\\nEstratégia 3: \\n %s \" % get_results(\"results_for_accuracy_calc_strategy_3.csv\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Estratégia 1: \n",
            "    accuracy      ppv       npv      sens      spec\n",
            "0  0.915254  0.92381  0.846154  0.979798  0.578947 \n",
            "\n",
            "Estratégia 2: \n",
            "    accuracy   ppv  npv  sens      spec\n",
            "0  0.991525  0.99  1.0   1.0  0.947368 \n",
            "\n",
            "Estratégia 3: \n",
            "    accuracy   ppv       npv      sens      spec\n",
            "0  0.932203  0.99  0.611111  0.933962  0.916667 \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}